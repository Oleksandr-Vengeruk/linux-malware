from pdfquery import PDFQuery # pdf
from bs4 import BeautifulSoup # html
import re
import os
import nltk # english words dataset 
from nltk.corpus import words
import string

nltk.download('words')

def check_nltk(word):
    return word.lower() in words.words()

# clean from punctuation, tabs and non-printable characters
def clean(text):
    text = text.replace('\t', ' ') # tabs
    punctuation = str.maketrans('', '', string.punctuation) #punctuation
    text = text.translate(punctuation)
    text = ''.join(char for char in text if char.isprintable()) #non-printable characters 
    return text

# separate english words 
def parse_text(text):
    text = clean(text)
    return [word.lower() for word in re.findall(r'\b\w+\b', text) if check_nltk(word)]

def extract_pdf(pdf_path):
    pdf = PDFQuery(pdf_path)
    pdf.load()
    text_elements = pdf.pq('LTTextLineHorizontal')
    text = ' '.join(t.text for t in text_elements)
    return parse_text(text)

def extract_html(html_path):
    with open(html_path, 'r', encoding='utf-8') as file:
        soup = BeautifulSoup(file, 'html.parser')
        text = soup.get_text()
    return parse_text(text)

# check file formt 
def file_type(input_path, output_path):
    if input_path.lower().endswith('.pdf'):
        words = extract_pdf(input_path)
    elif input_path.lower().endswith('.html'):
        words = extract_html(input_path)
    else:
        print("Unsupported file format")
        return
    total_words = len(words)

    with open(output_path, 'w', encoding='utf-8') as output_file:
        output_file.write(f"Total words: {total_words}\n\n")
        for word in words:
            output_file.write(word + '\n')

if __name__ == "__main__":
    input_file = r"repository\ ... \file.html/pdf" 
    output_file = r"repository\ ... \all-words.txt"

    file_type(input_file, output_file)
